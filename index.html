<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <title>Speech-to-IPA Prototype</title>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <style>
      body {
        font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
        margin: 0;
        padding: 3rem 1.5rem;
        background: #0f172a;
        color: #e2e8f0;
      }
      main {
        max-width: 720px;
        margin: 0 auto;
        background: rgba(15, 23, 42, 0.85);
        padding: 2.5rem;
        border-radius: 24px;
        box-shadow: 0 20px 45px rgba(15, 23, 42, 0.65);
      }
      h1 {
        margin-top: 0;
        font-size: 2.75rem;
        line-height: 1.1;
        color: #38bdf8;
      }
      p {
        line-height: 1.6;
      }
      code {
        background: rgba(226, 232, 240, 0.08);
        padding: 0.15rem 0.35rem;
        border-radius: 6px;
        font-size: 0.95rem;
      }
      pre {
        background: rgba(226, 232, 240, 0.08);
        padding: 1rem;
        border-radius: 12px;
        overflow-x: auto;
      }
      a {
        color: #f472b6;
      }
      ul {
        padding-left: 1.1rem;
      }
    </style>
  </head>
  <body>
    <main>
      <h1>Speech-to-IPA Prototype</h1>
      <p>
        This project is an experimental pipeline that stitches together
        <a href="https://github.com/guillaumekln/faster-whisper">faster-whisper</a>
        for multilingual transcription with the
        <a href="https://github.com/bootphon/phonemizer">phonemizer</a>
        toolkit (using <code>espeak-ng</code>) to emit IPA symbols. The current
        focus is on iterating quickly, validating IPA quality, and preparing for
        richer UI/feedback loops.
      </p>
      <p>
        Start by installing the Python dependencies and system packages, then run
        the CLI against any FFmpeg-compatible audio file:
      </p>
      <pre><code>python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
sudo apt-get install ffmpeg espeak-ng
python -m speechtoipa.cli example.wav --segments</code></pre>
      <p>
        The CLI outputs JSON (with <code>--output</code>) and human-readable
        summaries so we can plug it into future web or research workflows.
      </p>
      <p>
        Up next: streaming transcription, diarisation, richer suprasegmental
        modelling, and feedback capture tooling.
      </p>
    </main>
  </body>
</html>
